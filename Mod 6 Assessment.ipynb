{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center\">Module 6 Assessment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your Mod 6 Assessment. You will be tested for your understanding on concepts and ability to programmatically solve problems that have been covered in class and in the curriculum. Topics in this assessment include graph theory, natural language processing, and neural networks. \n",
    "\n",
    "You will have up to 90 minutes to complete this assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Natural Language Processing Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this exercise we will attempt to classify text messages as \"SPAM\" or \"HAM\" using TF-IDF Vectorization. Complete the functions below and answer the question(s) at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Import necessary libraries (we've imported some for you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_messages = pd.read_csv('data/spam.csv', usecols=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Convert string labels to 1 or 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_messages['target'] = le.fit_transform(df_messages['v1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 1) Create a function that takes in a string and returns a list of tokens or words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    Tokenizes a string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string: str object\n",
    "        String object to tokenize\n",
    "    Returns\n",
    "    --------\n",
    "    tokens : list\n",
    "        A list containing each word in string as an element \n",
    "\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 2) Create a function to remove stopwords and punctuation from a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens): \n",
    "    '''\n",
    "    Removes stopwords from a list of tokens (words)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: list object\n",
    "        List of tokens that need stopwords removed\n",
    "    Returns\n",
    "    --------\n",
    "    stopwords_removed : list object\n",
    "        A list containing tokens with stopwords removed\n",
    "\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Apply tokenization and stop word removal to our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_messages['tokens'] = df_messages['v2'].apply(lambda x: tokenize(x))\n",
    "df_messages['stopwords_removed'] = df_messages['tokens'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Most Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 3) Create a function that outputs the frequency of the n most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def frequency_distribution(tokens, n):\n",
    "    '''\n",
    "    Get n most common words in a Series of tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: pandas.Series object\n",
    "        Pandas series of tokens \n",
    "    n : int object\n",
    "        Integer defining the number of most common words to return\n",
    "    Returns\n",
    "    --------\n",
    "    most_common : list object\n",
    "        An array of tuples containing word frequency for n most common words\n",
    "\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "frequency_distribution(df_messages['stopwords_removed'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 4) Generate tf-idf vectorization for our data (split data into train and test here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tfidf(X, y): \n",
    "    '''\n",
    "    Generate train and test TF-IDF vectorization for our data set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas.Series object\n",
    "        Pandas series of text documents to classify \n",
    "    y : pandas.Series object\n",
    "        Pandas series containing label for each document\n",
    "    Returns\n",
    "    --------\n",
    "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n",
    "        Vector representation of train data\n",
    "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n",
    "        Vector representation of test data\n",
    "    y_train : array-like object\n",
    "        labels for training data\n",
    "    y_test : array-like object\n",
    "        labels for testing data\n",
    "    vectorizer : vectorizer object\n",
    "        fit TF-IDF vecotrizer object\n",
    "\n",
    "    '''\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf_idf_train, tf_idf_test, y_train, y_test, vecotorizer = tfidf(df_messages['v2'], df_messages['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Initialize classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 5) Create a function that takes in a classifier and trains it on our tf-idf vectors and generates test and train predictiions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    '''\n",
    "    Train a classifier to identify whether a message is spam or ham\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: sklearn classifier\n",
    "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n",
    "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n",
    "        TF-IDF vectorization of train data\n",
    "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n",
    "        TF-IDF vectorization of test data\n",
    "    y_train : pandas.Series object\n",
    "        Pandas series containing label for each document in the train set\n",
    "    Returns\n",
    "    --------\n",
    "    train_preds :  list object\n",
    "        Predictions for train data\n",
    "    test_preds :  list object\n",
    "        Predictions for test data\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate predictions for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_train_preds, nb_test_preds = classify_text(nb_classifier,tf_idf_train, tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, nb_test_preds))\n",
    "print(accuracy_score(y_test, nb_test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate predictions for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_train_preds, rf_test_preds = classify_text(rf_classifier,tf_idf_train, tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, rf_test_preds))\n",
    "print(accuracy_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function returns the word with the highest TF-IDF value in a given documnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_max_tf_idf(tf_idf, doc):\n",
    "    '''\n",
    "    Get word with highest TF-IDF value in a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tf_idf : spare matrix \n",
    "        TF-IDF vectorization of text data\n",
    "    doc : int object\n",
    "        Index of document in vectorization to get max tf-idf for\n",
    "    --------\n",
    "    max_tf_idf : str object\n",
    "        Word with highest TF-IDF value in a document\n",
    "    '''\n",
    "    x = tf_idf[doc].toarray()\n",
    "    max_tf_idf = vecotorizer.get_feature_names()[np.where(x[0] == max(x[0]))[0][0]]\n",
    "    return max_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 6) The word school(s) has the highest TF-IDF value in the second document of our test data. What does that tell us about the word school? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_max_tf_idf(tf_idf_test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Network Analysis Assessment\n",
    "\n",
    "For these next questions, you'll be using a graph dataset of facebook users and networkx. In the next cell, we're going to read in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.read_edgelist('./data/0.edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 1) Create a function `find_centrality` that returns a dictionary with the user with the highest betweenness centrality and the user with the highest degree centrality. It should return a dictionary that looks like:\n",
    "\n",
    "\n",
    "{'bc' : |user|, 'dc' : |user|}\n",
    "\n",
    "How does each of these people wield influence on the network? Imagine a message had to get to people from different communities. Who would be the best user to deliver the message to ensure that people from opposite communities receive the message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_centrality(graph):\n",
    "    \"\"\"\n",
    "    Calculates the most central nodes on a graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: networkx Graph object\n",
    "        Graph object to be analyzed\n",
    "    Returns\n",
    "    --------\n",
    "    centrality_dict : dict\n",
    "        A dictionary with the highest ranked user based off degree centrality and betweenness centrality \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 2) A marketing group is looking to target different communities with advertisements based off of their assumed mutual interests. Use the k_cliques_communities method to calculate the number of cliques formed with k users in a function `find_k_communities`. Calculate how many communities there are if the minimum size of a clique is 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_k_communities(graph,k):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: networkx Graph object\n",
    "        \n",
    "    k : int\n",
    "        k-number of connections required for a clique\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    num_communities: int\n",
    "        The number of communities present in the graph\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Assessment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning portion of this assessment is split into three main sections.  First, concepts from the introduction to deep learning are assessed by reconstructing the basic building blocks of a neural network.  Then, forward and back-propagation will be discussed in the “Multilayer Perceptron” section, as we build out a fully functioning neural network.\n",
    "\n",
    "Finally, you will be tuning and optimizing two neural networks trained on data generated with SKLearn — the first with regularization, and the second by modifying different aspects of the gradient descent process for deep learning.  You will receive credit for explaining your steps well even if the model does not improve much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_gaussian_quantiles, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential, regularizers\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>The Sigmoid Function</b></center>\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Given inputs and weights 1 through l and the sigmoid function (given above), write a function which computes the output y. Assume bias = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input_function):\n",
    "    \"\"\"\n",
    "    Transforms an input using the sigmoid function given above\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_function: function or numeric input to be transformed\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    output : float\n",
    "        result of the application of the sigmoid function \n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(x,w,b=1):\n",
    "    \"\"\"\n",
    "    Caluclates the perceptron output. Should use sigmoid as a helper function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        perceptron inputs\n",
    "    w : np.array\n",
    "        perceptron input weights\n",
    "    b : float\n",
    "        bias term\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    y : float\n",
    "        final output of the perceptron\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) How does this relate to logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Name two other activation functions and write functions for them as done with the sigmoid in part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/Deeper_network_day2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward propagation\n",
    "\n",
    "$ Z^{[l]}= W^{[l]} A^{[l-1]} + b^{[l]}$\n",
    "\n",
    "$ A^{[l]}= g^{[l]} ( Z^{[l]})$\n",
    "\n",
    "##### Back-propagation\n",
    "$ dZ^{[l]}= dA ^{[l]} * g^{[l]'} (Z^{[l]})$\n",
    "\n",
    "$ dW^{[l]} = \\dfrac{1}{m} dZ^{[l]}* A^{[l-1]T}$\n",
    "\n",
    "$ db^{[l]} = \\dfrac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True)$\n",
    "\n",
    "$ dA^{[l-1]} = W^{[l]T}*dZ^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Describe the process of forward propagation in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) How does what happens in forward-propagation change what happens in back-propagation? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Why is the chain rule important for backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7) You are training a neural network to pick out particular sounds in a dataset of audio files. Assume all preprocessing has already been done. If there are several sounds in each mp3 file, how would you modify your output layer to identify whether a particular sound occurs? How does your interpretation change assuming more than one sound can be in each file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and Optimization of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets are created using SKLearn, and should be improved. Although changing the number of nodes and layers may improve the models, focus on regularization in the first dataset, and gradient descent in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=450, noise=0.12)\n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'teal', 1:'orange'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    if key != 2:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: The following model is over-fit. Modify the following code to address the discrepancy between train and test accuracy.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test/split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8) Modify the code below to use L2 regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code goes in the cell below. Try running once without regularization first and look at what happens to train and test accuracy.\n",
    "\n",
    "Hint: use the activity_regularizer parameter in both of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "#Instantiate Classifier\n",
    "classifier = Sequential()\n",
    "\n",
    "#Hidden Layer\n",
    "classifier.add(Dense(\n",
    "    32, \n",
    "    activation='relu', \n",
    "    input_dim=2,\n",
    "    kernel_initializer='random_normal',\n",
    "\n",
    "))\n",
    "\n",
    "#Hidden Layer\n",
    "classifier.add(Dense(\n",
    "    32,\n",
    "    activation='relu', \n",
    "    input_dim=2,\n",
    "    kernel_initializer='random_normal',\n",
    "\n",
    "))\n",
    "\n",
    "#Output Layer\n",
    "classifier.add(Dense(\n",
    "    1, \n",
    "    activation='sigmoid',\n",
    "    kernel_initializer='random_uniform',\n",
    "))\n",
    "\n",
    "classifier.compile(optimizer ='adam',loss=\"binary_crossentropy\",metrics =['accuracy'])\n",
    "\n",
    "classifier.fit(X_train, y_train, epochs=25, verbose=0, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look what happens to train and test accuracy as you modify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "#predict classes\n",
    "predicted_vals_train = classifier.predict_classes(X_train)\n",
    "#show accuracy score\n",
    "print(accuracy_score(y_train,predicted_vals_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEST\n",
    "\n",
    "#predict classess\n",
    "predicted_vals_test = classifier.predict_classes(X_test)\n",
    "#show accuracy score\n",
    "print(accuracy_score(y_test,predicted_vals_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9) Explain what you did and how it changed the train and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff // "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10) Explain what regularization does, and how it affects the final weights of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11) How does L1 regularization change a neural network's architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with Gradient Descent\n",
    "\n",
    "A 3 dimensional dataset is generated using SKlearn and a poorly fit neural network is fit to it. Try improving the model using what's available through Keras, and then explain what you did in part 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/data.png' width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 3d data with complex error surface for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Construct dataset\n",
    "# Gaussian 1\n",
    "X1, y1 = make_gaussian_quantiles(cov=3.,\n",
    "                                 n_samples=10000, n_features=3,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X1 = pd.DataFrame(X1,columns=['x','y','z'])\n",
    "y1 = pd.Series(y1)\n",
    "\n",
    "# Gaussian 2\n",
    "X2, y2 = make_gaussian_quantiles(mean=(4, 4,2), cov=1,\n",
    "                                 n_samples=5000, n_features=3,\n",
    "                                 n_classes=2, random_state=2)\n",
    "X2 = pd.DataFrame(X2,columns=['x','y','z'])\n",
    "y2 = pd.Series(y2)\n",
    "# Combine the gaussians\n",
    "X1.shape\n",
    "X2.shape\n",
    "X = pd.DataFrame(np.concatenate((X1, X2)))\n",
    "y = pd.Series(np.concatenate((y1, - y2 + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12) Modify the code below to improve the starter model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: use help(Dense) to see what parameters you can change. You should be able to explain how these parameters relate to gradient descent. Don't worry too much about overfitting in this example, just focus on gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='zero', input_dim=3))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='zero'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='zero'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network, and specifying to measure accuracy at each step\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the neural network\n",
    "classifier.fit(X,y, batch_size=10, epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13) Explain why modifying the gradient descent process does anything and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// write stuff //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotdog",
   "language": "python",
   "name": "hotdog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
